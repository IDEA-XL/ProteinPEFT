setting:
  seed: 20000812
  os_environ:
    WANDB_API_KEY: 12219d65989b2bf7f80fd3a4d260194a713a6d01
    WANDB_RUN_ID: ~
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    MASTER_ADDR: localhost
    MASTER_PORT: 12315
    WORLD_SIZE: 1
    NODE_RANK: 0
  wandb_config:
    project: HumanPPI
    name: esm2_t33_650M_UR50D

model:
#    Which model to use
  model_py_path: esm/esm_ppi_model
  kwargs:
#    Arguments to initialize the specific class
    config_path: weights/PLMs/esm2_t33_650M_UR50D
    load_pretrained: True
    use_lora: True
    lora_config:
      task_type: SEQ_CLS
      target_modules: ["query", "key"]
      inference_mode: false
      lora_dropout: 0.1
      lora_alpha: 8
      r: 8

#    Arguments to initialize the basic class AbstractModel
  lr_scheduler_kwargs:
    last_epoch: -1
    init_lr: 1.e-3
    head_lr: 3.e-2
    lora_lr: 1.e-2
#    Weather to use this scheduler or not
    on_use: false

  optimizer_kwargs:
    betas: [0.9, 0.98]
    weight_decay: 0.01

  save_path: weights/HumanPPI/esm2_t33_650M_UR50D_lora


dataset:
#    Arguments to initialize the basic class LMDBDataset
  dataset_py_path: esm/esm_ppi_dataset
  dataloader_kwargs:
    batch_size: 1
    num_workers: 8

  train_lmdb: LMDB/HumanPPI/normal/train
  valid_lmdb: LMDB/HumanPPI/normal/valid
  test_lmdb: LMDB/HumanPPI/normal/test
#    Arguments to initialize the specific class
  kwargs:
    tokenizer: weights/PLMs/esm2_t33_650M_UR50D


#  Arguments to initialize Pytorch Lightning Trainer
Trainer:
  max_epochs: 1
  # max_steps: 2000
  limit_train_batches: 2000
  log_every_n_steps: 1
  strategy:
    find_unused_parameters: True
  logger: False
  enable_checkpointing: false
  # val_check_interval: 1.0
  val_check_interval: 500
  accelerator: gpu
  devices: 4
  num_nodes: 1
  accumulate_grad_batches: 16
  precision: 16
  num_sanity_val_steps: 0